{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# importing packages\n",
    "import pandas as pd\n",
    "\n",
    "#dvierse\n",
    "from string import ascii_letters\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing\n",
    "from pprint import pprint\n",
    "\n",
    "# for prediciting\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "\n",
    "\n",
    "# for assessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# importing data\n",
    "df_raw = pd.read_csv('C:/Users/Marc/Dropbox/06_ESCP/01_Uni/05_Term 3/04_HDI/01_Code/01_Input/210517_syn_data.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# drop not necessary index\n",
    "df_raw = df_raw.drop(columns = ['Unnamed: 0','CONTRACT_ID'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Approach:\n",
    "- Dataset understanding\n",
    "    - Split between Contract info, Customer info, property info, contract part, product description, damage description, damage target\n",
    "    - Continuous vs Categorical Data\n",
    "    - Multicollinearity?\n",
    "- Check Cleaning & Scale\n",
    "    - Data received clean, hence no cleaning to be performed\n",
    "        - MinMaxScaler\n",
    "        - StandardScaler\n",
    "        - RobustScaler\n",
    "        - Outlier reduction based on z-score\n",
    "        - Bucketing for Neural Network\n",
    "\n",
    "- Creation of Algorithms (Tried using lazy predict)\n",
    "    - Multilinear Regression\n",
    "    - XG Boost\n",
    "    - Random Forest\n",
    "    - Multilayer Perceptron\n",
    "    - Lasso Regression\n",
    "\n",
    "- Parameter Tuning (What do we want to optimize)\n",
    "    - GridSearchCV\n",
    "    - RandomizedSearchCV\n",
    "- Conclusion Discussion and Interpretation\n",
    "\n",
    "- More: Azure documentation as appendix\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_for_corr = df_raw.drop(columns = ['DAMAGE_TARGET'])\n",
    "# Multicolinearity\n",
    "\n",
    "# Correltation Heatmap\n",
    "sns.set_theme(style=\"white\")\n",
    "\n",
    "# Compute the correlation matrix\n",
    "corr = df_for_corr.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Filtering out the columns that have a correlation of more than 0.6\n",
    "abs_corr = corr.abs()\n",
    "abs_corr_filtered = abs_corr[abs_corr > 0.5]\n",
    "abs_corr_filtered.loc[:, (abs_corr_filtered.sum() > 2.2)].columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### Only Contract Info 1 has an multicolinearity of more than 0.6 (0.6 + 0.6 + 1, because of with itself and the two parts of the trinangle)\n",
    "### Drop it\n",
    "\n",
    "df_raw = df_raw.drop(columns=['CONTRACT_INFO_1'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_raw.describe()\n",
    "\n",
    "### mean and standard deviation are not around 0, hence scaling has to be done"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Selecting all numerical features to scale\n",
    "numerical_to_scale = ['CONTRACT_INFO_0','CONTRACT_INFO_7', 'CUSTOMER_INFO_1', 'CUSTOMER_INFO_2', 'CUSTOMER_INFO_3', 'DAMAGE_DESCRIPTION_0', 'DAMAGE_DESCRIPTION_1' ]\n",
    "num_to_s = df_raw[numerical_to_scale]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Exploration for features with continuous data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# boxpltos\n",
    "boxp = sns.boxplot(data=num_to_s,palette=\"colorblind\")\n",
    "boxp.set_xticklabels(boxp.get_xticklabels(),rotation=90)\n",
    "\n",
    "### hence devation must be very high"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# check devation\n",
    "num_to_s.describe()\n",
    "\n",
    "### Scaling to be done"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split X from y, as target variable should remain original values and not adjusted to the scale\n",
    "y = df_raw.DAMAGE_TARGET.values\n",
    "X = df_raw.drop(columns = ['DAMAGE_TARGET'])\n",
    "X_not_num = X.drop(columns=numerical_to_scale)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# to add columns back after scaling, as when scaling the column names are lost\n",
    "columns = num_to_s.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Standard Scaler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### No Scaling - bad option as data is not normal distributed\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "### Standard Scaler\n",
    "scaler_ss = StandardScaler()\n",
    "df_ss_num = pd.DataFrame(scaler_ss.fit_transform(num_to_s))\n",
    "df_ss_num.columns = columns\n",
    "\n",
    "# SS_boxpltos\n",
    "boxp_ss = sns.boxplot(data=df_ss_num,palette=\"colorblind\")\n",
    "boxp_ss.set_xticklabels(boxp_ss.get_xticklabels(),rotation=90)\n",
    "\n",
    "# add back to non numerical\n",
    "df_ss = pd.concat([df_ss_num, X_not_num], axis = 1)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train_ss, X_test_ss, y_train_ss, y_test_ss = train_test_split(df_ss, y, test_size=0.3, random_state=42)\n",
    "\n",
    "### Still outliers for Contract_info_7 and Customer_info 1,2,3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Robust Scaler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler_rs = RobustScaler()\n",
    "df_rs_num = pd.DataFrame(scaler_rs.fit_transform(num_to_s))\n",
    "df_rs_num.columns = columns\n",
    "\n",
    "# RS_boxpltos\n",
    "boxp_rs = sns.boxplot(data=df_rs_num,palette=\"colorblind\")\n",
    "boxp_rs.set_xticklabels(boxp_rs.get_xticklabels(),rotation=90)\n",
    "\n",
    "# add back to non numerical\n",
    "df_rs = pd.concat([df_rs_num, X_not_num], axis = 1)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train_rs, X_test_rs, y_train_rs, y_test_rs = train_test_split(df_rs, y, test_size=0.3, random_state=42)\n",
    "\n",
    "### Still outliers for Contract_info_7 and Customer_info 1,2,3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Min max Scaled"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler_mm = MinMaxScaler()\n",
    "df_min_max_num = pd.DataFrame(scaler_mm.fit_transform(num_to_s))\n",
    "df_min_max_num.columns = columns\n",
    "\n",
    "# MM_boxpltos\n",
    "boxp_mm = sns.boxplot(data=df_min_max_num,palette=\"colorblind\")\n",
    "boxp_mm.set_xticklabels(boxp_mm.get_xticklabels(),rotation=90)\n",
    "\n",
    "# add back to non numerical\n",
    "df_min_max = pd.concat([df_min_max_num, X_not_num], axis = 1)\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train_mm, X_test_mm, y_train_mm, y_test_mm = train_test_split(df_min_max, y, test_size=0.3, random_state=42)\n",
    "\n",
    "### Still outliers for Contract_info_7 and Customer_info 1,2,3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Z-score approach\n",
    "As with standard scaling methods, outliers remain. New approach of Z-Score\n",
    "https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n",
    "\n",
    "The Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "z = np.abs(stats.zscore(num_to_s))\n",
    "# to delete outliers\n",
    "df_z_score = num_to_s[(z < 2).all(axis=1)]\n",
    "\n",
    "### Standard Scaler\n",
    "scaler_zs = StandardScaler()\n",
    "df_zs_num = pd.DataFrame(scaler_zs.fit_transform(df_z_score))\n",
    "df_zs_num.columns = columns\n",
    "\n",
    "# ZS_boxpltos\n",
    "boxp_zs = sns.boxplot(data=df_zs_num,palette=\"colorblind\")\n",
    "boxp_zs.set_xticklabels(boxp_zs.get_xticklabels(),rotation=90)\n",
    "\n",
    "### Usally take z = 3 as like that around 99% of the data should be covered. When looked at the boxplots, there are however still outliers. Hence, approach could either adjust z further, or try predicition without customer info 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# add back to non numerical\n",
    "df_zs = df_zs_num.join(X_not_num, how = 'left')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train_ss, X_test_ss, y_train_ss, y_test_ss = train_test_split(df_ss, y, test_size=0.3, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Outliers very hard to scale, hence furhter apporach analyzed:\n",
    "- Bucketing of continuous features into ranges\n",
    "- Display them as integers\n",
    "- Run NN, that will simply adjust weights of nodes accordingly (almost none for outliers)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Bucketing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# objective to optimize q, as this defines number of buckets\n",
    "\n",
    "for i in numerical_to_scale:\n",
    "    X['quantile_' + i] = pd.qcut(X[i],q=1000, labels = False, duplicates= 'drop')\n",
    "\n",
    "X_final = X.drop(columns = numerical_to_scale)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#checking out valuecounts for different buckets - should outliers have their own buckets?\n",
    "X_final.quantile_DAMAGE_DESCRIPTION_1.value_counts().tail()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Multilayer Perceptron"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Mnual feature tuning by amount of bins (previous block of code) and the amount of hidden layers of neuron and also their amount)\n",
    "\n",
    "X_train_nn, X_test_nn, y_train_nn, y_test_nn = train_test_split(X_final, y,random_state=1)\n",
    "\n",
    "regr = MLPRegressor(random_state=1, max_iter=500, hidden_layer_sizes=(100,50,25)).fit(X_train_nn, y_train_nn)\n",
    "\n",
    "y_pred = regr.predict(X_test_nn)\n",
    "\n",
    "regr.score(X_test_nn, y_test_nn)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Simple other models with scaled data (no bucketing), as R2 of MLP is very low"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train_rs, y_train_rs)\n",
    "y_pred_lr = lr.predict(X_test_rs)\n",
    "lr.score(X_test_rs, y_test_rs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# XGboost"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# XGBoost does not work with objects\n",
    "# No features are objects\n",
    "# Approach: See which parameters worked best - add more parameters on the end which best param was found\n",
    "\n",
    "# Various hyper-parameters to tune\n",
    "xgb1 = XGBRegressor()\n",
    "xgb1.get_params()\n",
    "xgb1.fit(X_train_rs, y_train_rs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:linear'],\n",
    "              'learning_rate': [.02, .025, .03], #so called `eta` value\n",
    "              'max_depth': [3], #best\n",
    "              'min_child_weight': [2,2.5,3],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500]}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb1,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid.fit(X_train_rs,\n",
    "         y_train_rs)\n",
    "\n",
    "print(xgb_grid.best_score_)\n",
    "print(xgb_grid.best_params_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lasso Regression\n",
    "Approach:\n",
    "- Different alphas to see which would result in best results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lasso = Lasso()\n",
    "parameters = {'alpha':[1.4,1.45,1.5]}\n",
    "\n",
    "lasso_grid = GridSearchCV(lasso,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "\n",
    "lasso_grid.fit(X_train_rs,\n",
    "         y_train_rs)\n",
    "\n",
    "print(lasso_grid.best_score_)\n",
    "print(lasso_grid.best_params_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# RandomForrest Regressor\n",
    "https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(random_state = 42)\n",
    "rf.get_params()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "rf_param = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = rf_param, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Fit the random search model\n",
    "rf_random.fit(X_train_rs, y_train_rs)\n",
    "rf_random.best_params_\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lasso Regression to choose features based on coefficient\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Applying the best params of GridCV above\n",
    "\n",
    "from sklearn import linear_model\n",
    "clf = linear_model.Lasso(alpha=1.45)\n",
    "clf.fit(X_train_rs,\n",
    "         y_train_rs)\n",
    "Lasso(alpha=0.1)\n",
    "importance = np.abs(clf.coef_)\n",
    "\n",
    "feature_names = np.array(X_train_rs.columns)\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Filtering out all features with coefficiants above 20 (manually chosen)\n",
    "coeff = pd.DataFrame(importance).T\n",
    "coeff.columns = X_train_rs.columns\n",
    "coeff_important = coeff.T[coeff.T >= 20].dropna().T\n",
    "new_cols_20 = coeff_important.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lasso_20 = Lasso(alpha = 1.45).fit(X_train_rs[new_cols_20], y_train_rs)\n",
    "lasso_20.score(X_test_rs[new_cols_20], y_test_rs)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Applying the chose features from lasso and perform XGBoost\n",
    "xgb20 = XGBRegressor()\n",
    "\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:linear'],\n",
    "              'learning_rate': [.02, .025, .03], #so called `eta` value\n",
    "              'max_depth': [3], #best\n",
    "              'min_child_weight': [2,2.5,3],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500]}\n",
    "\n",
    "xgb_grid_20 = GridSearchCV(xgb20,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid_20.fit(X_train_rs[new_cols_20],\n",
    "         y_train_rs)\n",
    "\n",
    "print(xgb_grid_20.best_score_)\n",
    "print(xgb_grid_20.best_params_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Filtering out all features with coefficiants above 50 (manually chosen)\n",
    "coeff = pd.DataFrame(importance).T\n",
    "coeff.columns = X_train_rs.columns\n",
    "coeff_important = coeff.T[coeff.T >= 50].dropna().T\n",
    "new_cols_50 = coeff_important.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lasso_50 = Lasso(alpha = 1.45).fit(X_train_rs[new_cols_50], y_train_rs)\n",
    "lasso_50.score(X_test_rs[new_cols_50], y_test_rs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Applying the chose features from lasso and perform XGBoost\n",
    "xgb50 = XGBRegressor()\n",
    "\n",
    "parameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n",
    "              'objective':['reg:linear'],\n",
    "              'learning_rate': [.02, .025, .03], #so called `eta` value\n",
    "              'max_depth': [3], #best\n",
    "              'min_child_weight': [2,2.5,3],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.7],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [500]}\n",
    "\n",
    "xgb_grid_50 = GridSearchCV(xgb50,\n",
    "                        parameters,\n",
    "                        cv = 2,\n",
    "                        n_jobs = 5,\n",
    "                        verbose=True)\n",
    "\n",
    "xgb_grid_50.fit(X_train_rs[new_cols_50],\n",
    "         y_train_rs)\n",
    "\n",
    "print(xgb_grid_50.best_score_)\n",
    "print(xgb_grid_50.best_params_)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "MLP_score = [regr.score(X_test_nn, y_test_nn)]\n",
    "overview_test = pd.DataFrame(MLP_score, columns = ['MLP_score'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "MLP_score_train = [regr.score(X_train_nn, y_train_nn)]\n",
    "overview_train = pd.DataFrame(MLP_score_train, columns = ['MLP_score'])\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [
    "xgb_hyp_50_score = [xgb_grid_50.score(X_test_rs[new_cols_50], y_test_rs)]\n",
    "overview_test['xgb_hyp_50_score'] = pd.DataFrame(xgb_hyp_50_score)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [],
   "source": [
    "xgb_hyp_50_score_train = [xgb_grid_50.score(X_train_rs[new_cols_50], y_train_rs)]\n",
    "overview_train['xgb_hyp_50_score'] = pd.DataFrame(xgb_hyp_50_score_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [],
   "source": [
    "xgb_hyp_20_score = [xgb_grid_20.score(X_test_rs[new_cols_20], y_test_rs)]\n",
    "overview_test['xgb_hyp_20_score'] = pd.DataFrame(xgb_hyp_20_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [],
   "source": [
    "xgb_hyp_20_score_train = [xgb_grid_20.score(X_train_rs[new_cols_20], y_train_rs)]\n",
    "overview_train['xgb_hyp_20_score'] = pd.DataFrame(xgb_hyp_20_score_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [],
   "source": [
    "xgb_hyp_score = [xgb_grid.score(X_test_rs, y_test_rs)]\n",
    "overview_test['xgb_hyp_score'] = pd.DataFrame(xgb_hyp_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [
    "xgb_hyp_score_train = [xgb_grid.score(X_train_rs, y_train_rs)]\n",
    "overview_train['xgb_hyp_score'] = pd.DataFrame(xgb_hyp_score_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [],
   "source": [
    "xgb_score = [xgb1.score(X_test_rs, y_test_rs)]\n",
    "overview_test['xgb_score'] = pd.DataFrame(xgb_score, columns = ['xgb_score'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [],
   "source": [
    "xgb_score_train = [xgb1.score(X_train_rs, y_train_rs)]\n",
    "overview_train['xgb_score'] = pd.DataFrame(xgb_score_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [],
   "source": [
    "lasso_hyp_50_score = [lasso_50.score(X_test_rs[new_cols_50], y_test_rs)]\n",
    "overview_test['lasso_hyp_50_score'] = pd.DataFrame(lasso_hyp_50_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [
    "lasso_hyp_50_score_train = [lasso_50.score(X_train_rs[new_cols_50], y_train_rs)]\n",
    "overview_train['lasso_hyp_50_score'] = pd.DataFrame(lasso_hyp_50_score_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [],
   "source": [
    "lasso_hyp_20_score = [lasso_20.score(X_test_rs[new_cols_20], y_test_rs)]\n",
    "overview_test['lasso_hyp_20_score'] = pd.DataFrame(lasso_hyp_20_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [],
   "source": [
    "lasso_hyp_20_score_train = [lasso_20.score(X_train_rs[new_cols_20], y_train_rs)]\n",
    "overview_train['lasso_hyp_20_score'] = pd.DataFrame(lasso_hyp_20_score_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [],
   "source": [
    "lasso_hyp_score = [lasso_grid.score(X_test_rs, y_test_rs)]\n",
    "overview_test['lasso_hyp_score'] = pd.DataFrame(lasso_hyp_score)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [],
   "source": [
    "lasso_hyp_score_train = [lasso_grid.score(X_train_rs, y_train_rs)]\n",
    "overview_train['lasso_hyp_score'] = pd.DataFrame(lasso_hyp_score_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [],
   "source": [
    "rf_hyp_score = [rf_random.score(X_test_rs, y_test_rs)]\n",
    "overview_test['rf_hyp_score'] = pd.DataFrame(rf_hyp_score)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [],
   "source": [
    "rf_hyp_score_train = [rf_random.score(X_train_rs, y_train_rs)]\n",
    "overview_train['rf_hyp_score'] = pd.DataFrame(rf_hyp_score_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [],
   "source": [
    "lr_score = [lr.score(X_test_rs, y_test_rs)]\n",
    "overview_test['lr_score'] = pd.DataFrame(lr_score, columns = ['lr_score'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "outputs": [],
   "source": [
    "lr_score_train = [lr.score(X_train_rs, y_train_rs)]\n",
    "overview_train['lr_score'] = pd.DataFrame(lr_score)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [],
   "source": [
    "overview_test = overview_test.T.rename(columns = {0:'overview_test'})\n",
    "overview_train = overview_train.T.rename(columns = {0:'overview_train'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [],
   "source": [
    "overview = overview_train.join(overview_test, how = 'left')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [
    {
     "data": {
      "text/plain": "                    overview_train  overview_test\nMLP_score                 0.019184       0.015341\nxgb_hyp_50_score          0.037779       0.032072\nxgb_hyp_20_score          0.049091       0.039977\nxgb_hyp_score             0.072682       0.055294\nxgb_score                 0.269324       0.022227\nlasso_hyp_50_score        0.023906       0.023907\nlasso_hyp_20_score        0.026382       0.027225\nlasso_hyp_score           0.028869       0.029547\nrf_hyp_score              0.437097       0.046946\nlr_score                  0.029216       0.029216",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>overview_train</th>\n      <th>overview_test</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>MLP_score</th>\n      <td>0.019184</td>\n      <td>0.015341</td>\n    </tr>\n    <tr>\n      <th>xgb_hyp_50_score</th>\n      <td>0.037779</td>\n      <td>0.032072</td>\n    </tr>\n    <tr>\n      <th>xgb_hyp_20_score</th>\n      <td>0.049091</td>\n      <td>0.039977</td>\n    </tr>\n    <tr>\n      <th>xgb_hyp_score</th>\n      <td>0.072682</td>\n      <td>0.055294</td>\n    </tr>\n    <tr>\n      <th>xgb_score</th>\n      <td>0.269324</td>\n      <td>0.022227</td>\n    </tr>\n    <tr>\n      <th>lasso_hyp_50_score</th>\n      <td>0.023906</td>\n      <td>0.023907</td>\n    </tr>\n    <tr>\n      <th>lasso_hyp_20_score</th>\n      <td>0.026382</td>\n      <td>0.027225</td>\n    </tr>\n    <tr>\n      <th>lasso_hyp_score</th>\n      <td>0.028869</td>\n      <td>0.029547</td>\n    </tr>\n    <tr>\n      <th>rf_hyp_score</th>\n      <td>0.437097</td>\n      <td>0.046946</td>\n    </tr>\n    <tr>\n      <th>lr_score</th>\n      <td>0.029216</td>\n      <td>0.029216</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overview"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}